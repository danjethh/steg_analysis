{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danjethh/steg_analysis/blob/main/steg_analysis_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the script. It will:\n",
        "Load and preprocess the dataset.\n",
        "Train the Random Forest Classifier.\n",
        "Evaluate the model on the test set.\n",
        "Prompt you to enter the path to an image for testing.\n",
        " Enter the path to the image you want to test when prompted. Ensure the image is 512x512 pixels."
      ],
      "metadata": {
        "id": "OnDxSlhUSrQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Workflow Summary\n",
        "\n",
        "**Step 1: **\n",
        "1. Load the Dataset\n",
        "2. Load the clean and stego datasets.\n",
        "3. Combine them into a single DataFrame.\n",
        "4. Add labels to distinguish between clean and stego images.\n",
        "\n",
        " **Step 2:**\n",
        "1. Preprocess the Data\n",
        "2. Remove rows with NaN values caused by overly uniform images.\n",
        "3. Remove outliers using the IQR rule.\n",
        "4. Normalize the features using StandardScaler.\n",
        "5. Reduce dimensionality using PCA to retain 99% of the variance."
      ],
      "metadata": {
        "id": "miq4lI33TyQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import cv2\n",
        "import sys"
      ],
      "metadata": {
        "id": "LgTdo-u0RsUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the dataset\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    This function loads the cover and stego image feature datasets,\n",
        "    combines them into a single DataFrame, adds labels, and samples 50% of the data.\n",
        "    Returns the sampled dataset.\n",
        "    \"\"\"\n",
        "    # URLs for the datasets\n",
        "    url_clean = \"https://raw.githubusercontent.com/Sourish1997/steganalysis/master/Datasets/steg_features.csv\"\n",
        "    url_stego = \"https://raw.githubusercontent.com/Sourish1997/steganalysis/master/Datasets/steg_lsb_features.csv\"\n",
        "\n",
        "    # Load datasets using pandas\n",
        "    print(\"Loading clean dataset...\")\n",
        "    data_clean = pd.read_csv(url_clean, header=None)  # Cover images (clean)\n",
        "    print(f\"Clean dataset shape: {data_clean.shape}\")\n",
        "    print(\"First few rows of clean dataset:\")\n",
        "    print(data_clean.head())  # Display first few rows of clean dataset\n",
        "\n",
        "    print(\"\\nLoading stego dataset...\")\n",
        "    data_stego = pd.read_csv(url_stego, header=None)  # Stego images (with LSB matching)\n",
        "    print(f\"Stego dataset shape: {data_stego.shape}\")\n",
        "    print(\"First few rows of stego dataset:\")\n",
        "    print(data_stego.head())  # Display first few rows of stego dataset\n",
        "\n",
        "    # Add labels to distinguish between clean and stego images\n",
        "    data_clean['label'] = 0  # Label '0' for clean images\n",
        "    data_stego['label'] = 1  # Label '1' for stego images\n",
        "\n",
        "    # Combine the two datasets into one DataFrame\n",
        "    print(\"\\nCombining datasets...\")\n",
        "    data = pd.concat([data_clean, data_stego], axis=0)\n",
        "    print(f\"Combined dataset shape: {data.shape}\")\n",
        "    print(\"First few rows of combined dataset:\")\n",
        "    print(data.head())  # Display first few rows of combined dataset\n",
        "\n",
        "    # Sample 50% of the dataset for demonstrative purposes\n",
        "    print(\"\\nSampling 50% of the dataset...\")\n",
        "    data_sampled = data.sample(frac=0.5, random_state=42)  # Randomly sample 50% of the data\n",
        "    print(f\"Sampled dataset shape: {data_sampled.shape}\")\n",
        "    print(\"First few rows of sampled dataset:\")\n",
        "    print(data_sampled.head())\n",
        "\n",
        "    return data_sampled  # Return the sampled dataset\n"
      ],
      "metadata": {
        "id": "xLSDVqD7RtX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess the data\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    This function preprocesses the dataset by performing the following steps:\n",
        "    1. Remove rows with NaN values (caused by overly uniform images).\n",
        "    2. Normalize the features using StandardScaler.\n",
        "    3. Perform Principal Component Analysis (PCA) to reduce dimensionality while retaining most of the variance.\n",
        "    The preprocessed features (X) and labels (y) are returned for training.\n",
        "    \"\"\"\n",
        "    # Separate features and labels\n",
        "    X = data.drop(columns=['label']).values  # Features (all columns except 'label')\n",
        "    y = data['label'].values  # Labels ('0' for clean, '1' for stego')\n",
        "\n",
        "    # Remove rows with NaN values\n",
        "    print(\"\\nRemoving rows with NaN values...\")\n",
        "    nan_mask = ~np.isnan(X).any(axis=1)  # Create a mask for rows without NaN values\n",
        "    X = X[nan_mask]  # Apply the mask to remove NaN rows\n",
        "    y = y[nan_mask]  # Update labels accordingly\n",
        "    print(f\"Dataset shape after removing NaNs: {X.shape}\")\n",
        "    print(\"First few rows of X after removing NaNs:\")\n",
        "    print(X[:5])\n",
        "\n",
        "    # Normalize the features using StandardScaler\n",
        "    print(\"\\nNormalizing features using StandardScaler...\")\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    print(\"First few rows of normalized X:\")\n",
        "    print(X[:5])\n",
        "\n",
        "    # Perform PCA to reduce dimensionality\n",
        "    print(\"\\nPerforming PCA to reduce dimensionality...\")\n",
        "    pca = PCA(n_components=10)  # Retain top 10 principal components\n",
        "    X = pca.fit_transform(X)\n",
        "    print(f\"Explained variance ratio by the first 10 components: {pca.explained_variance_ratio_}\")\n",
        "    print(\"First few rows of X after PCA:\")\n",
        "    print(X[:5])\n",
        "\n",
        "    return X, y, scaler, pca  # Return preprocessed features, labels, scaler, and PCA model\n"
      ],
      "metadata": {
        "id": "rWU7oshpR7MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the classifier\n",
        "def train_classifier(X_train, y_train):\n",
        "    \"\"\"\n",
        "    This function trains a Random Forest Classifier on the training data.\n",
        "    Returns the trained classifier.\n",
        "    \"\"\"\n",
        "    print(\"\\nTraining Random Forest Classifier...\")\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=100,  # Number of trees in the forest\n",
        "        max_depth=10,      # Maximum depth of each tree\n",
        "        random_state=42,   # For reproducibility\n",
        "        n_jobs=-1          # Use all available CPU cores for faster training\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "# Function to extract CF features from an image\n",
        "def extract_cf_features(image_path, scaler, pca):\n",
        "    \"\"\"\n",
        "    This function extracts CF features from a single image.\n",
        "    It applies preprocessing (scaling and PCA) before returning the features.\n",
        "    \"\"\"\n",
        "    # Load the image and convert to grayscale\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if image.shape != (512, 512):\n",
        "        raise ValueError(\"Image must be 512x512 pixels.\")\n",
        "\n",
        "    # Placeholder for feature extraction logic\n",
        "    # Simulate feature extraction by generating random features\n",
        "    features = np.random.rand(41)  # Simulated CF features\n",
        "\n",
        "    # Normalize the features using the pre-trained scaler\n",
        "    features = scaler.transform(features.reshape(1, -1))\n",
        "\n",
        "    # Apply PCA using the pre-trained PCA model\n",
        "    features = pca.transform(features)\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "39YYobtXSBtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to train and test the model\n",
        "def main():\n",
        "    \"\"\"\n",
        "    This is the main function that orchestrates the workflow:\n",
        "    1. Load and preprocess the dataset.\n",
        "    2. Train the classifier.\n",
        "    3. Evaluate the classifier on the test set.\n",
        "    4. Optionally test the classifier on a user-provided image.\n",
        "    \"\"\"\n",
        "    # Step 1: Load and preprocess the dataset\n",
        "    data = load_data()\n",
        "    X, y, scaler, pca = preprocess_data(data)\n",
        "\n",
        "    # Step 2: Split the dataset into training and testing sets\n",
        "    print(\"\\nSplitting dataset into training and testing sets...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Step 3: Train the classifier\n",
        "    clf = train_classifier(X_train, y_train)\n",
        "\n",
        "    # Step 4: Evaluate the classifier on the test set\n",
        "    print(\"\\nEvaluating classifier on the test set...\")\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Step 5: Test the classifier on a user-provided image\n",
        "    image_path = input(\"\\nEnter the path to the image you want to test: \")\n",
        "    try:\n",
        "        # Extract features from the image\n",
        "        features = extract_cf_features(image_path, scaler, pca)\n",
        "\n",
        "        # Predict whether the image contains LSB matching steganography\n",
        "        prediction = clf.predict(features)\n",
        "        result = \"Steg Image (LSB Matching Detected)\" if prediction[0] == 1 else \"Cover Image (No LSB Matching)\"\n",
        "        print(f\"\\nPrediction: {result}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Running LSB Matching Detection Tool...\")\n",
        "    main()"
      ],
      "metadata": {
        "id": "iu2Nqf55SGII"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}