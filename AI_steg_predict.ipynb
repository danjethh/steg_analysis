{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danjethh/steg_analysis/blob/main/AI_steg_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the script. It will:\n",
        "Load and preprocess the dataset.\n",
        "Train the Random Forest Classifier.\n",
        "Evaluate the model on the test set.\n",
        "Prompt you to enter the path to an image for testing.\n",
        " Enter the path to the image you want to test when prompted. Ensure the image is 512x512 pixels."
      ],
      "metadata": {
        "id": "OnDxSlhUSrQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part A: Train binary classification model"
      ],
      "metadata": {
        "id": "WNA3XonFORo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Workflow Summary\n",
        "\n",
        "**Step 1:**\n",
        "1. Load the clean dataset - 10,000 features dataset\n",
        "2. Load the stego datasets - 10,000 features dataset\n",
        "3. Combine them into a single DataFrame.\n",
        "4. Add labels to distinguish between clean and stego images.\n",
        "\n",
        "**Step 2:**\n",
        "Preprocess the combined 20,000 dataset\n",
        "1. Removing rows containing NaN values (invalid computations or uniform features).\n",
        "2. Normalizing features using StandardScaler (to ensure zero mean and unit variance).\n",
        "3. Reducing dimensionality using PCA (retains 10 most important components).\n",
        "4. Train classifier\n",
        "5. Extract CF features from image provided for prediction"
      ],
      "metadata": {
        "id": "miq4lI33TyQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-RQEgFpEQqj",
        "outputId": "8a492abc-b5e3-4151-f2b6-d2aa42724095"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (2.0.2)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m166.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "LgTdo-u0RsUy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load the Dataset\n",
        "\n",
        "### Input\n",
        "- No external input from the user.\n",
        "- Internally fetches two datasets from URLs:\n",
        "  - Clean image features (`steg_features.csv`)\n",
        "  - Stego image features (`steg_lsb_features.csv`)\n",
        "\n",
        "### Output\n",
        "- A **combined dataset** (DataFrame) containing 20,000 rows (10,000 clean + 10,000 stego) with:\n",
        "  - 41 feature columns\n",
        "  - 1 label column (`0` for clean, `1` for stego)\n",
        "\n",
        "### Brief Explanation\n",
        "This function loads pre-extracted feature vectors for clean and stego images. Each image is represented by **41 statistical and transformation-based features**. After loading:\n",
        "\n",
        "1. Clean images are labeled with `0`, and stego images with `1`.\n",
        "2. The datasets are then **concatenated into one unified table**.\n",
        "3. First 4 rows of each set and first 8 rows of the combined dataset are printed.\n",
        "4. The final dataset (20,000 rows × 42 columns) is returned for further preprocessing and machine learning.\n",
        "\n",
        "This helps students see what raw feature data looks like **before any processing or training begins**.\n"
      ],
      "metadata": {
        "id": "BfJtn_siPdgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2:\n",
        "def load_data():\n",
        "\n",
        "    # URLs for clean and stego datasets (CSV with 41 features each)\n",
        "    url_clean = \"https://raw.githubusercontent.com/Sourish1997/steganalysis/master/Datasets/steg_features.csv\"\n",
        "    url_stego = \"https://raw.githubusercontent.com/Sourish1997/steganalysis/master/Datasets/steg_lsb_features.csv\"\n",
        "\n",
        "    # Load clean (cover) images feature dataset\n",
        "    print(\"Loading clean (cover) dataset...\")\n",
        "    data_clean = pd.read_csv(url_clean, header=None)\n",
        "    data_clean['label'] = 0  # Label '0' for clean images\n",
        "\n",
        "    # Display first 4 rows of the 10,000 rows\n",
        "    print(\"\\nFirst 4 rows from Clean (Cover) Dataset:\")\n",
        "    print(data_clean.head(4))\n",
        "\n",
        "    # Load stego images feature dataset\n",
        "    print(\"\\nLoading stego dataset...\")\n",
        "    data_stego = pd.read_csv(url_stego, header=None)\n",
        "    data_stego['label'] = 1  # Label '1' for stego images\n",
        "\n",
        "    # Display first 4 rows of the 10,000 rows from stego dataset\n",
        "    print(\"\\nFirst 4 rows from Stego Dataset:\")\n",
        "    print(data_stego.head(4))\n",
        "\n",
        "    # Combine both datasets\n",
        "    print(\"\\nCombining clean and stego datasets into a single DataFrame...\")\n",
        "    data_combined = pd.concat([data_clean, data_stego], axis=0, ignore_index=True)\n",
        "\n",
        "    # Display first 8 rows of the combined dataset with labels\n",
        "    print(\"\\nFirst 8 rows of the Combined Dataset (including labels):\")\n",
        "    print(data_combined.head(8))\n",
        "\n",
        "    # Display the shape of the combined dataset\n",
        "    print(f\"\\nCombined Dataset Shape: {data_combined.shape}\")\n",
        "\n",
        "    return data_combined  # Return full dataset (100%) without sampling\n",
        "\n",
        "# Run the function\n",
        "full_dataset = load_data()"
      ],
      "metadata": {
        "id": "xLSDVqD7RtX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb0cd271-55b6-470e-9725-bae3e60acfbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading clean (cover) dataset...\n",
            "\n",
            "First 4 rows from Clean (Cover) Dataset:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.317327  0.827515  0.760605  0.740966  0.721418  0.910647  0.861356   \n",
            "1       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "2 -0.503111  0.862970  0.802899  0.775813  0.751000  0.927452  0.889261   \n",
            "3 -0.182988  0.887022  0.835196  0.813357  0.789932  0.911072  0.861291   \n",
            "\n",
            "          7         8         9  ...        32        33        34        35  \\\n",
            "0  0.835196  0.815543  0.818339  ... -0.004257 -0.000239 -0.266943 -0.106837   \n",
            "1       NaN       NaN       NaN  ... -0.064528  0.015347  0.005049 -0.145678   \n",
            "2  0.866067  0.848226  0.855546  ...  0.003529  0.009316 -0.248362 -0.107545   \n",
            "3  0.824739  0.795830  0.856713  ... -0.024424  0.004261 -0.137704 -0.088573   \n",
            "\n",
            "         36        37        38        39        40  label  \n",
            "0 -0.059703 -0.015162 -0.006729 -0.004329  0.001190      0  \n",
            "1 -0.189235  0.075486  0.015123 -0.066373  0.015776      0  \n",
            "2 -0.072559 -0.018520 -0.014878  0.004437  0.008626      0  \n",
            "3 -0.171084  0.064899  0.052169 -0.024969 -0.000246      0  \n",
            "\n",
            "[4 rows x 42 columns]\n",
            "\n",
            "Loading stego dataset...\n",
            "\n",
            "First 4 rows from Stego Dataset:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.317249  0.827004  0.760402  0.740601  0.721245  0.909903  0.861059   \n",
            "1       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "2 -0.503030  0.862545  0.802756  0.775952  0.750876  0.927202  0.888695   \n",
            "3 -0.182986  0.885745  0.833919  0.812366  0.788900  0.909339  0.859644   \n",
            "\n",
            "          7         8         9  ...        32        33        34        35  \\\n",
            "0  0.834736  0.815029  0.817973  ...  0.001464  0.000648 -0.260819 -0.115387   \n",
            "1       NaN       NaN       NaN  ... -0.044616  0.013183 -0.042575 -0.151737   \n",
            "2  0.865893  0.847715  0.855019  ...  0.005700  0.006849 -0.249954 -0.110913   \n",
            "3  0.823534  0.794631  0.855730  ... -0.020100  0.004419 -0.156632 -0.101755   \n",
            "\n",
            "         36        37        38        39        40  label  \n",
            "0 -0.062382 -0.013157 -0.004018  0.000808  0.001451      1  \n",
            "1 -0.159439  0.056094  0.011527 -0.049579  0.011257      1  \n",
            "2 -0.068477 -0.015880 -0.013919  0.002656  0.007633      1  \n",
            "3 -0.147277  0.049512  0.043812 -0.021355 -0.000414      1  \n",
            "\n",
            "[4 rows x 42 columns]\n",
            "\n",
            "Combining clean and stego datasets into a single DataFrame...\n",
            "\n",
            "First 8 rows of the Combined Dataset (including labels):\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.317327  0.827515  0.760605  0.740966  0.721418  0.910647  0.861356   \n",
            "1       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "2 -0.503111  0.862970  0.802899  0.775813  0.751000  0.927452  0.889261   \n",
            "3 -0.182988  0.887022  0.835196  0.813357  0.789932  0.911072  0.861291   \n",
            "4  0.006107  0.932943  0.906990  0.897635  0.886993  0.970490  0.954652   \n",
            "5 -0.062837  0.842912  0.754166  0.697135  0.650435  0.856007  0.775904   \n",
            "6  0.158983  0.938860  0.902474  0.878589  0.856160  0.939188  0.904426   \n",
            "7 -0.506522  0.850555  0.747610  0.678859  0.628110  0.848701  0.759642   \n",
            "\n",
            "          7         8         9  ...        32        33        34        35  \\\n",
            "0  0.835196  0.815543  0.818339  ... -0.004257 -0.000239 -0.266943 -0.106837   \n",
            "1       NaN       NaN       NaN  ... -0.064528  0.015347  0.005049 -0.145678   \n",
            "2  0.866067  0.848226  0.855546  ...  0.003529  0.009316 -0.248362 -0.107545   \n",
            "3  0.824739  0.795830  0.856713  ... -0.024424  0.004261 -0.137704 -0.088573   \n",
            "4  0.944758  0.934639  0.934616  ... -0.009279 -0.015531 -0.130431 -0.099206   \n",
            "5  0.732342  0.700388  0.774753  ... -0.015302 -0.010715 -0.129836 -0.147590   \n",
            "6  0.881622  0.861823  0.918068  ... -0.014055 -0.009330 -0.088094 -0.139422   \n",
            "7  0.711028  0.672725  0.793992  ... -0.019033 -0.003773 -0.111305 -0.113080   \n",
            "\n",
            "         36        37        38        39        40  label  \n",
            "0 -0.059703 -0.015162 -0.006729 -0.004329  0.001190      0  \n",
            "1 -0.189235  0.075486  0.015123 -0.066373  0.015776      0  \n",
            "2 -0.072559 -0.018520 -0.014878  0.004437  0.008626      0  \n",
            "3 -0.171084  0.064899  0.052169 -0.024969 -0.000246      0  \n",
            "4 -0.136827  0.039127  0.025786 -0.013232 -0.022728      0  \n",
            "5 -0.092916 -0.005466 -0.006112 -0.016379 -0.015106      0  \n",
            "6 -0.135156  0.030703  0.008192 -0.010647 -0.015093      0  \n",
            "7 -0.167413  0.074449  0.054960 -0.019523 -0.011951      0  \n",
            "\n",
            "[8 rows x 42 columns]\n",
            "\n",
            "Combined Dataset Shape: (20000, 42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Preprocess the Dataset\n",
        "\n",
        "### Input\n",
        "- `data`: A DataFrame combining both clean and stego images with 41 feature columns and a `label` column.\n",
        "\n",
        "### Output\n",
        "- `X`: Processed feature matrix (after cleaning, normalization, and PCA).\n",
        "- `y`: Corresponding labels (`0` = clean, `1` = stego).\n",
        "- `scaler`: Fitted `StandardScaler` object (used to normalize future input data).\n",
        "- `pca`: Fitted PCA object (used to reduce future data to 10 key components).\n",
        "\n",
        "### Brief Explanation\n",
        "This function prepares the dataset for machine learning by performing the following steps:\n",
        "\n",
        "1. **Remove invalid rows**: Any row with a missing or undefined value (NaN) is dropped.\n",
        "2. **Normalize features**: Standardizes each feature so it has a mean of 0 and standard deviation of 1. This ensures equal treatment of all features.\n",
        "3. **Dimensionality Reduction (PCA)**: Compresses the 41 feature dimensions down to the top 10 principal components. These components capture the most important patterns in the data while reducing redundancy.\n",
        "\n",
        "This helps simplify the dataset, improve learning efficiency, and avoid overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "hR_TlDq9RJk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2:\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(data):\n",
        "\n",
        "    # Step 1: Split dataset into features and labels\n",
        "    X = data.drop(columns=['label']).values  # Drop label column for features\n",
        "    y = data['label'].values  # Extract label column\n",
        "\n",
        "    # Step 2: Remove any row that contains NaN (can happen due to uniform images or divide by zero)\n",
        "    print(\"\\nStep 1: Removing rows with NaN values...\")\n",
        "    nan_mask = ~np.isnan(X).any(axis=1)  # Mask where rows do not contain NaNs\n",
        "    X = X[nan_mask]\n",
        "    y = y[nan_mask]\n",
        "    print(f\"Dataset shape after removing NaNs: {X.shape}\")\n",
        "    print(\"First row of X (after NaN removal):\")\n",
        "    print(X[:1])  # Show first 5 rows\n",
        "\n",
        "    # Step 3: Normalize the features to have mean=0 and std=1\n",
        "    print(\"\\nStep 2: Normalizing features with StandardScaler...\")\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    print(\"First row after normalization:\")\n",
        "    print(X[:1])\n",
        "\n",
        "    # Step 4: Apply PCA to reduce to top 10 most significant components\n",
        "    print(\"\\n Step 3: Applying PCA to reduce dimensionality to 10 components...\")\n",
        "    pca = PCA(n_components=10)\n",
        "    X = pca.fit_transform(X)\n",
        "    print(\"Explained Variance Ratio of PCA:\")\n",
        "    print(pca.explained_variance_ratio_)\n",
        "    print(\"First row of transformed features after PCA:\")\n",
        "    print(X[:1])\n",
        "\n",
        "    return X, y, scaler, pca\n",
        "\n",
        "# Run the preprocessing function\n",
        "X_processed, y_labels, scaler_model, pca_model = preprocess_data(full_dataset)\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_labels, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "rWU7oshpR7MB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb66708-ddd9-4ad8-d29f-b91b75018601"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1: Removing rows with NaN values...\n",
            "Dataset shape after removing NaNs: (19358, 41)\n",
            "First row of X (after NaN removal):\n",
            "[[-3.17326879e-01  8.27515384e-01  7.60604845e-01  7.40965975e-01\n",
            "   7.21417838e-01  9.10647001e-01  8.61356432e-01  8.35196209e-01\n",
            "   8.15543437e-01  8.18339071e-01  7.58361024e-01  7.32744946e-01\n",
            "   7.12313450e-01  8.06705401e-01  7.59407793e-01  1.60797637e-01\n",
            "   2.14583946e-01  1.91990069e-01  1.82255482e-01  1.76242578e-01\n",
            "  -2.84393560e-01 -1.14700224e-01 -5.35871230e-02 -8.57756000e-04\n",
            "  -2.30281000e-04 -4.37930600e-03  3.93935000e-03 -2.54409068e-01\n",
            "  -1.58545180e-01 -4.41293100e-02 -6.17937400e-03 -1.58812800e-03\n",
            "  -4.25697300e-03 -2.38575000e-04 -2.66942911e-01 -1.06837230e-01\n",
            "  -5.97025860e-02 -1.51620180e-02 -6.72895500e-03 -4.32901300e-03\n",
            "   1.18994800e-03]]\n",
            "\n",
            "Step 2: Normalizing features with StandardScaler...\n",
            "First row after normalization:\n",
            "[[-0.48827255 -0.12484325 -0.06925128  0.02812186  0.08114797  0.34235018\n",
            "   0.35863645  0.38018801  0.40450614  0.05288872  0.11872946  0.19616453\n",
            "   0.24778305  0.19400538  0.01681015 -7.28024259 -7.27339814 -5.05339646\n",
            "  -4.26411698 -3.79384445 -2.84843326  1.91783086  0.73084139 -0.38490332\n",
            "  -0.2208824   0.47365171  0.18328555 -2.26185187  0.01086307  1.10218498\n",
            "  -0.53798699 -0.33881461  0.57937272  0.02336896 -2.41625949  0.64025244\n",
            "   0.99844767 -0.66786054 -0.44988043  0.58640967  0.32221075]]\n",
            "\n",
            " Step 3: Applying PCA to reduce dimensionality to 10 components...\n",
            "Explained Variance Ratio of PCA:\n",
            "[0.34933847 0.25325881 0.11328639 0.10396695 0.0582953  0.03268833\n",
            " 0.01998358 0.01416521 0.01266699 0.00843501]\n",
            "First row of transformed features after PCA:\n",
            "[[ 0.44492168  3.4831905  -8.31895093 -7.87979839 -3.35021102  4.92507816\n",
            "  -1.56227855  0.15574811 -3.23956005 -0.36786936]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Train the Classifier (Random Forest)\n",
        "\n",
        "### Input\n",
        "- `X_train`: Feature matrix for training (output from `train_test_split`)\n",
        "- `y_train`: Label array corresponding to `X_train` (`0 = Cover`, `1 = Stego`)\n",
        "\n",
        "### Output\n",
        "- `clf`: Trained Random Forest Classifier model.\n",
        "\n",
        "### Brief Explanation\n",
        "This function uses the training portion of the preprocessed dataset to build a machine learning model that can detect steganography.\n",
        "\n",
        "1. **Random Forest Classifier**:\n",
        "   - An ensemble-based model that combines many decision trees to improve accuracy and avoid overfitting.\n",
        "   - We use 100 trees (`n_estimators=100`) and limit each tree to a maximum depth of 10 (`max_depth=10`).\n",
        "\n",
        "2. **Model Training**:\n",
        "   - The classifier is trained using the `fit()` function on the training data.\n",
        "\n",
        "3. **Evaluation on Training Set**:\n",
        "   - **Training Accuracy**: Measures how well the model performs on the training data.\n",
        "   - **Classification Report**: Provides detailed performance for each class (`Cover`, `Stego`) including:\n",
        "     - Precision: How many predicted positives were actually correct.\n",
        "     - Recall: How many actual positives were correctly predicted.\n",
        "     - F1-score: Harmonic mean of precision and recall.\n"
      ],
      "metadata": {
        "id": "wPsq4CBRR9hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4:\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def train_classifier(X_train, y_train):\n",
        "    print(\"\\nTraining Random Forest Classifier...\")\n",
        "\n",
        "    # Initialize classifier\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on training set to evaluate performance\n",
        "    train_preds = clf.predict(X_train)\n",
        "    train_accuracy = accuracy_score(y_train, train_preds)\n",
        "    train_report = classification_report(y_train, train_preds, target_names=[\"Cover\", \"Stego\"])\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report on Training Set:\")\n",
        "    print(train_report)\n",
        "\n",
        "    return clf, train_accuracy, train_report\n",
        "\n",
        "clf, accuracy, report = train_classifier(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vJ1ji3o-Eyj",
        "outputId": "a88e8c9b-57c6-49d0-9567-d9786fed0eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Random Forest Classifier...\n",
            "\n",
            "Training Accuracy: 0.7801\n",
            "\n",
            "Classification Report on Training Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Cover       0.90      0.63      0.74      6759\n",
            "       Stego       0.72      0.93      0.81      6791\n",
            "\n",
            "    accuracy                           0.78     13550\n",
            "   macro avg       0.81      0.78      0.78     13550\n",
            "weighted avg       0.81      0.78      0.78     13550\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B: Predict whether or not image is original or Steg"
      ],
      "metadata": {
        "id": "eaWyr9VhOlJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Load two images(Original and Steg Image)"
      ],
      "metadata": {
        "id": "B8JBFG-xOzDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load two images"
      ],
      "metadata": {
        "id": "Z8veGZP2O-r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Extract 10 features for a given image\n",
        "\n",
        "- **Input:**\n",
        "  - `image_array`: A grayscale image in 2D format (512x512 pixels).\n",
        "  - `scaler`: A trained StandardScaler model used to normalize the 41 features.\n",
        "  - `pca`: A trained PCA model used to reduce the 41 features to 10.\n",
        "\n",
        "- **Output:**\n",
        "  - `pca_features`: A 10-value vector used in training/classification.\n",
        "  - `raw_features`: A 41-value feature vector containing all extracted features for analysis.\n",
        "\n",
        "- **Purpose:**\n",
        "  This function extracts 41 statistical and correlation-based features from an image to detect steganography. The full feature vector is normalized and reduced using PCA to produce the 10 most important features for classification.\n",
        "\n",
        "- **Used In:**\n",
        "  Machine learning model training and prediction.\n"
      ],
      "metadata": {
        "id": "veFDbBDFXEb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract CF features from an image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "from scipy.stats import pearsonr\n",
        "import pywt\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def getPlaneBits(plane_id, binary_image):\n",
        "    return [int(b[plane_id]) for b in binary_image]\n",
        "\n",
        "def getBitPlanes(img):\n",
        "    bin_image = [np.binary_repr(pixel, width=8) for row in img for pixel in row]\n",
        "    bit_planes = [np.array(getPlaneBits(i, bin_image)).reshape(img.shape) for i in range(8)]\n",
        "    return bit_planes\n",
        "\n",
        "def autocor(matrix, k, l):\n",
        "    Xk = matrix[0:matrix.shape[0] - k, 0:matrix.shape[1] - l]\n",
        "    Xl = matrix[k:matrix.shape[0], l:matrix.shape[1]]\n",
        "    return pearsonr(Xk.flatten(), Xl.flatten())\n",
        "\n",
        "def getCHl(hist, l):\n",
        "    return pearsonr(hist[0:256 - l], hist[l:256])\n",
        "\n",
        "def getModifiedWavelet(coefficients, threshold):\n",
        "    coefficients[np.abs(coefficients) < threshold] = 0\n",
        "    return coefficients\n",
        "\n",
        "def getE(img, threshold):\n",
        "    LL, (LH, HL, HH) = pywt.dwt2(img, 'haar')\n",
        "    LH = getModifiedWavelet(LH, threshold)\n",
        "    HL = getModifiedWavelet(HL, threshold)\n",
        "    HH = getModifiedWavelet(HH, threshold)\n",
        "    img_denoised = pywt.idwt2((LL, (LH, HL, HH)), 'haar')\n",
        "    return img - img_denoised\n",
        "\n",
        "def getCE(img, threshold, k, l):\n",
        "    residual = getE(img, threshold)\n",
        "    return autocor(residual, k, l)\n",
        "\n",
        "# --- Main CF Extraction Function ---\n",
        "def extract_cf_features(image_array, scaler, pca):\n",
        "    \"\"\"\n",
        "    Extracts 41 correlation features (CF) from a given grayscale image array.\n",
        "    Then applies StandardScaler and PCA to reduce dimensionality for classification.\n",
        "\n",
        "    Parameters:\n",
        "    - image_array: 2D array of grayscale image (512x512)\n",
        "    - scaler: Trained StandardScaler for normalization\n",
        "    - pca: Trained PCA model for dimensionality reduction\n",
        "\n",
        "    Returns:\n",
        "    - pca_features: PCA-reduced feature vector (used for prediction)\n",
        "    - raw_features: Original 41 extracted features (for analysis or display)\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    bit_planes = getBitPlanes(image_array)\n",
        "\n",
        "    # Bit plane correlation\n",
        "    M1, M2 = bit_planes[0], bit_planes[1]\n",
        "    features.append(pearsonr(M1.flatten(), M2.flatten())[0])\n",
        "\n",
        "    # Autocorrelation on LSB\n",
        "    autocor_pairs = [\n",
        "        [1, 0], [2, 0], [3, 0], [4, 0],\n",
        "        [0, 1], [0, 2], [0, 3], [0, 4],\n",
        "        [1, 1], [2, 2], [3, 3], [4, 4],\n",
        "        [1, 2], [2, 1]\n",
        "    ]\n",
        "    for k, l in autocor_pairs:\n",
        "        features.append(autocor(M1, k, l)[0])\n",
        "\n",
        "    # Histogram-based correlations\n",
        "    img_hist, _ = np.histogram(image_array.flatten(), bins=256, density=True)\n",
        "    He = img_hist[::2]  # Even indexed bins\n",
        "    Ho = img_hist[1::2]  # Odd indexed bins\n",
        "    features.append(pearsonr(He, Ho)[0])\n",
        "\n",
        "    for i in range(1, 5):\n",
        "        features.append(getCHl(img_hist, i)[0])\n",
        "\n",
        "    # Wavelet residual correlations\n",
        "    wavelet_triplets = [\n",
        "        [1.5, 0, 1], [1.5, 1, 0], [1.5, 1, 1], [1.5, 0, 2], [1.5, 2, 0], [1.5, 1, 2], [1.5, 2, 1],\n",
        "        [2.0, 0, 1], [2.0, 1, 0], [2.0, 1, 1], [2.0, 0, 2], [2.0, 2, 0], [2.0, 1, 2], [2.0, 2, 1],\n",
        "        [2.5, 0, 1], [2.5, 1, 0], [2.5, 1, 1], [2.5, 0, 2], [2.5, 2, 0], [2.5, 1, 2], [2.5, 2, 1]\n",
        "    ]\n",
        "    for t, k, l in wavelet_triplets:\n",
        "        features.append(getCE(image_array, t, k, l)[0])\n",
        "\n",
        "    # Final transformations\n",
        "    raw_features = np.array(features)\n",
        "    scaled_features = scaler.transform(raw_features.reshape(1, -1))\n",
        "    pca_features = pca.transform(scaled_features)\n",
        "\n",
        "    print(\"\\nExtracted 41 Raw Features (Before PCA):\")\n",
        "    print(raw_features)\n",
        "    print(\"\\nTransformed 10 Features (After PCA):\")\n",
        "    print(pca_features)\n",
        "\n",
        "    return pca_features, raw_features\n"
      ],
      "metadata": {
        "id": "39YYobtXSBtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Predict whether an image contains embedded message using 10 features\n",
        "\n",
        "### Purpose  \n",
        "To test if a new image contains a hidden message using the trained AI model.\n",
        "\n",
        "### Input  \n",
        "- `scaler`: The trained scaler used during preprocessing.  \n",
        "- `pca`: The PCA model used to reduce the image features.  \n",
        "- `clf`: The trained Random Forest Classifier.\n",
        "\n",
        "### Output  \n",
        "- A printed label showing the prediction result:\n",
        "  - `\"Steg Image (LSB Matching Detected)\"` or  \n",
        "  - `\"Cover Image (No LSB Matching)\"`  \n",
        "- Also returns the prediction result as a string.\n",
        "\n",
        "### Description  \n",
        "This step allows a user to input the URL of a grayscale `.pgm` image (512x512). The image is downloaded, verified, and passed through the same feature extraction, scaling, and dimensionality reduction steps used during training.  \n",
        "The model then classifies the image based on its patterns and structure to determine if it contains hidden data.\n"
      ],
      "metadata": {
        "id": "W5_LSavhYOJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Prediction Function Using Extracted Features ---\n",
        "def run_prediction(scaler, pca, clf):\n",
        "    image_url = input(\"\\nEnter the URL of the image to test (must be 512x512 grayscale .pgm): \")\n",
        "    print(\"\\nDownloading and processing image...\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(image_url)\n",
        "        image_array = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
        "        image = cv2.imdecode(image_array, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if image is None or image.shape != (512, 512):\n",
        "            raise ValueError(\"The input image must be a 512x512 grayscale image.\")\n",
        "\n",
        "        print(\"Extracting CF features and making prediction...\")\n",
        "        pca_features, _ = extract_cf_features(image, scaler, pca)\n",
        "        prediction = clf.predict(pca_features)\n",
        "\n",
        "        result = \"Steg Image (LSB Matching Detected)\" if prediction[0] == 1 else \"Cover Image (No LSB Matching)\"\n",
        "        print(\"\\nPrediction Result:\", result)\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nError processing the image:\", e)\n",
        "        return None\n",
        "\n",
        "run_prediction(scaler_model, pca_model, clf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "iu2Nqf55SGII",
        "outputId": "cf727ec7-df52-4b25-cc8e-361e3a4ae98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter the URL of the image to test (must be 512x512 grayscale .pgm): https://raw.githubusercontent.com/Sourish1997/steganalysis/master/bossbase_lsb_sample/10.pgm\n",
            "\n",
            "Downloading and processing image...\n",
            "Extracting CF features and making prediction...\n",
            "\n",
            "Extracted 41 Raw Features (Before PCA):\n",
            "[ 2.58205319e-01  9.34991267e-01  9.03683538e-01  8.80693491e-01\n",
            "  8.63054141e-01  9.10866062e-01  8.62611929e-01  8.33683789e-01\n",
            "  8.09458513e-01  8.86023243e-01  8.34739012e-01  8.01282461e-01\n",
            "  7.73709520e-01  8.49065832e-01  8.68571071e-01  9.93773254e-01\n",
            "  9.92885348e-01  9.77747576e-01  9.55579513e-01  9.29936882e-01\n",
            " -1.61586108e-01 -2.21841262e-01 -5.99212646e-02  1.65894214e-03\n",
            "  3.50848929e-04 -4.18594897e-03 -3.48756891e-04 -1.71581772e-01\n",
            " -1.90686278e-01 -7.48615771e-02  3.75685823e-03  5.95338925e-03\n",
            " -4.90759264e-03  2.00567018e-03 -1.75854897e-01 -1.76790903e-01\n",
            " -7.89162097e-02  4.87498585e-03  8.31458405e-03 -9.00236267e-03\n",
            " -1.47567485e-03]\n",
            "\n",
            "Transformed 10 Features (After PCA):\n",
            "[[ 0.48811999  2.64872495  0.7506218   0.82174806 -0.85101093 -1.25945119\n",
            "   1.21182543 -0.18158708 -0.08878053 -0.36442942]]\n",
            "\n",
            "Prediction Result: Steg Image (LSB Matching Detected)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Steg Image (LSB Matching Detected)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Explanation of the Process:\n",
        "1. Input : The user provided the path to the link to the image for testing.\n",
        "2. Feature Extraction : The program extracted features from the image using the CF (Correlation Features) feature set described in the project report. These features capture spatial information from the image, particularly focusing on the least significant bit planes.\n",
        "3. Preprocessing : The extracted features were preprocessed to ensure compatibility with the trained model. This includes:\n",
        "\n",
        "  3.1 Normalization using StandardScaler.\n",
        "\n",
        "  3.2 Dimensionality reduction using Principal Component Analysis (PCA).\n",
        "4. Prediction : The preprocessed features were passed to the trained voting ensemble model, which consists of parameter-tuned versions of MLP Classifier and AdaBoost models.\n",
        "5. Output : The model predicted that the image does not contain LSB matching steganography, classifying it as a Cover Image .\n",
        "\n",
        "Key Points from the Output:\n",
        "1. Prediction : The model classified the image as a Cover Image , meaning no signs of LSB matching steganography were detected.\n",
        "2. Confidence : While the exact confidence score is not provided in the output, the model's accuracy and F-score (as reported in the project) suggest a reliable prediction. The final model achieved an accuracy of 75.52% and an F-score of 79.30% , which is significantly better than the benchmark Gaussian Naïve Bayes model.\n",
        "\n",
        " Possible Scenarios:\n",
        "1. True Negative : If the image is indeed a clean image without any steganography, the prediction is correct.\n",
        "2. False Negative : If the image contains LSB matching steganography but was misclassified as a cover image, this would indicate a limitation of the model. However, given the high F-score of the model, such cases are less likely but not impossible.\n",
        "\n",
        " Limitations to Consider:\n",
        "1. Image Size : The feature extraction process is designed for 512x512 grayscale images. If the input image does not meet this requirement, it may have been cropped or resampled, potentially affecting the prediction.\n",
        "2. Overly Uniform Images : If the image is overly dark or bright, some CF features may result in NaN values, making it unsuitable for analysis. However, since the program completed the prediction, this issue likely did not occur here."
      ],
      "metadata": {
        "id": "YE7SrnA72tsO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}