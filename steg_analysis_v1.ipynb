{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danjethh/steg_analysis/blob/main/steg_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Load the Dataset**"
      ],
      "metadata": {
        "id": "EhkC7Gp-v9Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Function to load the dataset\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    This function loads the cover and stego image feature datasets.\n",
        "    It combines the two datasets into a single DataFrame and adds labels:\n",
        "    - Label '0' for clean images (from steg_features.csv).\n",
        "    - Label '1' for stego images (from steg_lsb_features.csv).\n",
        "    The combined dataset is returned for further processing.\n",
        "    \"\"\"\n",
        "    # URLs for the datasets\n",
        "    url_clean = \"https://raw.githubusercontent.com/Sourish1997/steganalysis/master/Datasets/steg_features.csv\"\n",
        "    url_stego = \"https://raw.githubusercontent.com/Sourish1997/steganalysis/master/Datasets/steg_lsb_features.csv\"\n",
        "\n",
        "    # Load datasets using pandas\n",
        "    print(\"Loading clean dataset...\")\n",
        "    data_clean = pd.read_csv(url_clean, header=None)  # Cover images (clean)\n",
        "    print(f\"Clean dataset shape: {data_clean.shape}\")\n",
        "    print(\"First few rows of clean dataset:\")\n",
        "    print(data_clean.head())  # Display first few rows of clean dataset\n",
        "\n",
        "    print(\"\\nLoading stego dataset...\")\n",
        "    data_stego = pd.read_csv(url_stego, header=None)  # Stego images (with LSB matching)\n",
        "    print(f\"Stego dataset shape: {data_stego.shape}\")\n",
        "    print(\"First few rows of stego dataset:\")\n",
        "    print(data_stego.head())  # Display first few rows of stego dataset\n",
        "\n",
        "    # Add labels to distinguish between clean and stego images\n",
        "    data_clean['label'] = 0  # Label '0' for clean images\n",
        "    data_stego['label'] = 1  # Label '1' for stego images\n",
        "\n",
        "    # Combine the two datasets into one DataFrame\n",
        "    print(\"\\nCombining datasets...\")\n",
        "    data = pd.concat([data_clean, data_stego], axis=0)\n",
        "    print(f\"Combined dataset shape: {data.shape}\")\n",
        "    print(\"First few rows of combined dataset:\")\n",
        "    print(data.head())  # Display first few rows of combined dataset\n",
        "\n",
        "    return data  # Return the combined dataset\n",
        "\n",
        "# Run the function to load the data\n",
        "data = load_data()"
      ],
      "metadata": {
        "id": "AdSZSbxrwBtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Preprocess the Data**"
      ],
      "metadata": {
        "id": "BP6F-hhKwFdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Function to preprocess the data\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    This function preprocesses the dataset by performing the following steps:\n",
        "    1. Remove rows with NaN values (caused by overly uniform images).\n",
        "    2. Remove outliers using the IQR rule to improve model robustness.\n",
        "    3. Normalize the features using StandardScaler to ensure all features have zero mean and unit variance.\n",
        "    4. Perform Principal Component Analysis (PCA) to reduce dimensionality while retaining most of the variance.\n",
        "    The preprocessed features (X) and labels (y) are returned for training.\n",
        "    \"\"\"\n",
        "    # Separate features and labels\n",
        "    X = data.drop(columns=['label']).values  # Features (all columns except 'label')\n",
        "    y = data['label'].values  # Labels ('0' for clean, '1' for stego')\n",
        "\n",
        "    # Remove rows with NaN values\n",
        "    print(\"\\nRemoving rows with NaN values...\")\n",
        "    nan_mask = ~np.isnan(X).any(axis=1)  # Create a mask for rows without NaN values\n",
        "    X = X[nan_mask]  # Apply the mask to remove NaN rows\n",
        "    y = y[nan_mask]  # Update labels accordingly\n",
        "    print(f\"Dataset shape after removing NaNs: {X.shape}\")\n",
        "    print(\"First few rows of X after removing NaNs:\")\n",
        "    print(X[:5])\n",
        "\n",
        "    # Remove outliers using the IQR rule\n",
        "    def remove_outliers(X):\n",
        "        \"\"\"\n",
        "        This helper function removes outliers from the dataset using the Interquartile Range (IQR) rule.\n",
        "        An entry is considered an outlier if it lies outside the range [Q1 - 1.5*IQR, Q3 + 1.5*IQR].\n",
        "        \"\"\"\n",
        "        Q1 = np.percentile(X, 25, axis=0)  # First quartile (25th percentile)\n",
        "        Q3 = np.percentile(X, 75, axis=0)  # Third quartile (75th percentile)\n",
        "        IQR = Q3 - Q1  # Interquartile range\n",
        "        lower_bound = Q1 - 1.5 * IQR  # Lower bound for valid values\n",
        "        upper_bound = Q3 + 1.5 * IQR  # Upper bound for valid values\n",
        "        outlier_mask = np.all((X >= lower_bound) & (X <= upper_bound), axis=1)  # Mask for non-outliers\n",
        "        return X[outlier_mask], outlier_mask  # Return filtered data and mask\n",
        "\n",
        "    print(\"\\nRemoving outliers using IQR rule...\")\n",
        "    X, outlier_mask = remove_outliers(X)  # Remove outliers from features\n",
        "    y = y[outlier_mask]  # Update labels accordingly\n",
        "    print(f\"Dataset shape after removing outliers: {X.shape}\")\n",
        "    print(\"First few rows of X after removing outliers:\")\n",
        "    print(X[:5])\n",
        "\n",
        "    # Normalize the features using StandardScaler\n",
        "    print(\"\\nNormalizing features using StandardScaler...\")\n",
        "    scaler = StandardScaler()  # Initialize the scaler\n",
        "    X = scaler.fit_transform(X)  # Normalize features to have zero mean and unit variance\n",
        "    print(\"First few rows of normalized X:\")\n",
        "    print(X[:5])\n",
        "\n",
        "    # Perform PCA to reduce dimensionality\n",
        "    print(\"\\nPerforming PCA to reduce dimensionality...\")\n",
        "    pca = PCA(n_components=10)  # Retain 10 principal components (captures 99.23% variance)\n",
        "    X = pca.fit_transform(X)  # Transform the data to the reduced feature space\n",
        "    print(f\"Explained variance ratio by the first 10 components: {pca.explained_variance_ratio_}\")\n",
        "    print(\"First few rows of X after PCA:\")\n",
        "    print(X[:5])\n",
        "\n",
        "    return X, y  # Return preprocessed features and labels\n",
        "\n",
        "# Run the function to preprocess the data\n",
        "X, y = preprocess_data(data)"
      ],
      "metadata": {
        "id": "NnbEh3BzwH15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Train Classifiers**\n",
        "\n"
      ],
      "metadata": {
        "id": "ed9Di6ipwLol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "print(\"\\nSplitting data into training and testing sets...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Training set size: {X_train.shape}, Testing set size: {X_test.shape}\")\n",
        "\n",
        "# Function to train classifiers\n",
        "def train_classifiers(X_train, y_train):\n",
        "    \"\"\"\n",
        "    This function trains multiple classifiers on the training data.\n",
        "    It also performs hyperparameter tuning using GridSearchCV for some classifiers.\n",
        "    The best-performing models are returned as a dictionary.\n",
        "    \"\"\"\n",
        "    # Define classifiers\n",
        "    classifiers = {\n",
        "        'GaussianNB': GaussianNB(),  # Gaussian Naive Bayes (simple and fast)\n",
        "        'RandomForest': RandomForestClassifier(random_state=42),  # Random Forest (ensemble method)\n",
        "        'SVC': SVC(probability=True, random_state=42),  # Support Vector Classifier (handles high-dimensional data well)\n",
        "        'MLP': MLPClassifier(random_state=42, max_iter=500),  # Multi-Layer Perceptron classifier\n",
        "        'AdaBoost': AdaBoostClassifier(random_state=42)  # Adaptive Boosting (ensemble method)\n",
        "    }\n",
        "\n",
        "    # Hyperparameter grids for tuning\n",
        "    param_grids = {\n",
        "        'RandomForest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]},  # Number of trees and depth\n",
        "        'SVC': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},  # Regularization strength and kernel type\n",
        "        'MLP': {'hidden_layer_sizes': [(50,), (100,)], 'alpha': [0.0001, 0.001]},  # Network architecture and regularization\n",
        "        'AdaBoost': {'n_estimators': [50, 100], 'learning_rate': [0.1, 1.0]}  # Number of estimators and learning rate\n",
        "    }\n",
        "\n",
        "    best_models = {}  # Dictionary to store the best models\n",
        "    for name, clf in classifiers.items():\n",
        "        print(f\"\\nTraining {name} classifier...\")\n",
        "        if name in param_grids:\n",
        "            # Perform hyperparameter tuning using GridSearchCV\n",
        "            print(f\"Tuning hyperparameters for {name}...\")\n",
        "            grid = GridSearchCV(clf, param_grids[name], cv=5, scoring='f1')  # Use 5-fold cross-validation\n",
        "            grid.fit(X_train, y_train)  # Fit the model to the training data\n",
        "            best_models[name] = grid.best_estimator_  # Store the best estimator\n",
        "            print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "        else:\n",
        "            # Train the classifier without hyperparameter tuning\n",
        "            clf.fit(X_train, y_train)  # Fit the model to the training data\n",
        "            best_models[name] = clf  # Store the trained model\n",
        "            print(f\"{name} trained successfully.\")\n",
        "\n",
        "    return best_models  # Return the dictionary of best models\n",
        "\n",
        "# Run the function to train classifiers\n",
        "best_models = train_classifiers(X_train, y_train)"
      ],
      "metadata": {
        "id": "zsgNms1jwP-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Build Voting Ensemble**"
      ],
      "metadata": {
        "id": "E6g1_JlZwTFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Function to build voting ensemble\n",
        "def build_voting_ensemble(best_models, X_train, y_train):\n",
        "    \"\"\"\n",
        "    This function builds a voting ensemble of the best-performing models.\n",
        "    The ensemble uses soft voting, which averages the predicted probabilities from each model.\n",
        "    The ensemble is trained on the training data and returned.\n",
        "    \"\"\"\n",
        "    print(\"\\nBuilding voting ensemble...\")\n",
        "    # Create a list of (name, model) tuples for the ensemble\n",
        "    estimators = [(name, model) for name, model in best_models.items()]\n",
        "\n",
        "    # Initialize the voting classifier with soft voting\n",
        "    voting_clf = VotingClassifier(estimators, voting='soft')\n",
        "\n",
        "    # Train the ensemble on the training data\n",
        "    print(\"Training voting ensemble...\")\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "\n",
        "    return voting_clf  # Return the trained ensemble\n",
        "\n",
        "# Run the function to build the voting ensemble\n",
        "voting_clf = build_voting_ensemble(best_models, X_train, y_train)"
      ],
      "metadata": {
        "id": "58S3gDSpwal-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Evaluate the Model**"
      ],
      "metadata": {
        "id": "dw26ID1Xwcw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    This function evaluates the performance of the model on the test data.\n",
        "    It calculates the accuracy and F-score and returns them as metrics.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating the model...\")\n",
        "    # Make predictions on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Predictions on test data: {y_pred[:10]}\")  # Display first 10 predictions\n",
        "\n",
        "    # Calculate accuracy and F-score\n",
        "    accuracy = accuracy_score(y_test, y_pred)  # Accuracy: fraction of correct predictions\n",
        "    f_score = f1_score(y_test, y_pred)  # F-score: harmonic mean of precision and recall\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F-Score: {f_score:.4f}\")\n",
        "\n",
        "    return accuracy, f_score  # Return the evaluation metrics\n",
        "\n",
        "# Run the function to evaluate the model\n",
        "accuracy, f_score = evaluate_model(voting_clf, X_test, y_test)"
      ],
      "metadata": {
        "id": "qYGw-h2jwiZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
